# Итоговый проект 4 модуля по дисциплине "ETL-процессы"

## Датасет

За основу взят датасет [Mental Health Dataset](https://www.kaggle.com/datasets/bhavikjikadara/mental-health-dataset), объемом в 300k строк и шириной в 17 столбцов. Исходные данные были получены в .csv-формате.

|Поле|Тип|Смысловое значение|
|-|-|-|
|timestamp|Date|Дата опроса|
|gender|String|Пол|
|country|String|Страна проживания|
|occupation|String|Род деятельности|
|self_employeed|Boolean|Самозанятость|
|family_history|Boolean|Информация о наличии ментальных проблем в семье|
|treatment|Boolean|Обращался ли опрашиваемый за лечением ментальных проблем|
|days_indoors|String|Сколько дней может не выходить из дома|
|growing_stress|String|Растет ли уровень стресса|
|changes_habbits|String|Была ли смена привычек|
|mental_health_history|String|Были ли раньше проблемы с ментальным здоровьем|
|mood_swings|String|Наблюдаются ли скачки настроения|
|coping_struggles|Boolean|Накапливаются ли трудности|
|work_interest|String|Есть ли интерес к работе|
|social_weakness|String|Наблюдается ли социальная слабость|
|mental_health_interview|String|Проводились ли опросы по ментальному здоровью|
|care_options|String|Доступны ли методы ухода за собой в текущем состоянии|

## Задание 1: Работа с Yandex DataTransfer

### Создание YDB
Для начала создадим YDB и таблицу с соответсвующими датасету типами полей. Она была создана через UI, поэтому здесь будет пропущен скрипт ее создания. Кроме того переименуем `timestamp` в `answer_date` и добавим поле `id` для облегчения идентификации.

![ydb](/final_4/inc/ydb.png)

### Подготовка данных
 Для загрузки данных в базу необходимо провести ряд изменений и трансформаций, так как, например, некоторые значения из датасета не подходят под значения типов YDB, строки не проиндексированы. Скрипт приведен в файле [task_1\transform_1.py](/final_4/task_1/transform_1.py).

После этого загрузим данные из нашего файла в YDB командой [task_1\ydb.sh](/final_4/task_1/ydb.sh). К сожалению, при выгрузке всего объема данных `ydb cli` выдавал ошибку (на скрине ниже), поэтому объем данных был сокращен до 100 тысяч записей.

![ydb_err](/final_4/inc/ydb_error.png)

Данные успешно загрузились в скорректированном объеме.

![ydb_suc](/final_4/inc/ydb_success.png)

### Создание трансфера
Когда данные оказались в базе, можно приступать к созданию Data Transfer'а. Создадим его c эндпоинтами источника в YDB и приемника в Object Storage. Как видим, он завершил свою работу успешно.

![data_transfer](/final_4/inc/trnsfr_done.png)

### Проверка результата
Проверим работоспособность нашего трансфера. Для этого перейдем в бакет Object Storage и найдем файл, в который данные должны были загрузиться. Находим его по пути `2025\06\05\mental_health.csv`

![os_1](/final_4/inc/os_1.png)

Проверим содержимое файла - выгрузка прошла успешно

![os_2](/final_4/inc/os_2.png)

## Задание 2: Автоматизация работы с Yandex Data Processing при помощи Apache Airflow

### Подготовка инфраструктуры

Для начала создадим инстанс Airflow.

![af](/final_4/inc/af.png)

И кластер Data Processing.

![dproc](/final_4/inc/dproc.png)

### Подготовка PySpark-задания

### Подготовка и запуск DAG-файла

## Задание 3: Работа с топиками Apache Kafka с помощью PySpark-заданий в Yandex Data Processing

### Подготовка архитектуры

Создадим инстанс Kafka-кластера и топик `mh_changes` в нем, а также не забудем создать пользователя с доступом к данным.

![kafka](/final_4/inc/kafka.png)

![kafka-top](/final_4/inc/kafka-topic.png)

![kafka-user](/final_4/inc/kafka-user.png)

В задании кроме этого воспользуемся кластером Data Processing, созданным в предыдущем задании.

### Создание PySpark-заданий

За основу возьмем тот же датафрейм из 100 тысяч записей, прочитаем его из Object Storage внутри PySpark-задания для создания фрейма данных.
Скрипт загрузки данных находится в [task_3/kafka-write.py](/final_4/task_3/kafka-write.py). Для удобства дальнейшего обращения к данным была реализована spark-схема. 

На основе этого файла запустим задание и дождемся его выполнения. После нескольких попыток запуска видим статус успеха.

![kafka-write](/final_4/inc/kafka-write.png)

Для того, чтобы проверить наличие этих данных в топике необходимо создать PySpark-задание на чтение, его скрипт находится в [task_3/kafka-read.py](/final_4/task_3/kafka-read.py). Оно отработало успешно, теперь проверим наличие файлов в s3.

![kafka-read](/final_4/inc/kafka-read.png)

Наблюдаем в Object Storage в папке `kafka-read-stream-output` файл `SUCCESS` и наши прочитанные данные, разбитые на 2 части, содержимое в которых совпадает с теми данными, которые мы загружали в топик.

![kafka-spark-out](/final_4/inc/kafka-spark-out.png)

![kafka-txt](/final_4/inc/kafka-txt.png)

По порядку следования идентификаторов записей можоно сделать вывод, что некоторые записи могли попадать в dlq-топик и после повторной обработки успешно считывались.